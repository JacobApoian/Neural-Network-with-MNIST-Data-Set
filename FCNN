import torch
import torchvision
from torchvision import datasets,transforms
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import torch.optim as optim
import matplotlib.pyplot as plt

# google drive
from google.colab import drive
drive.mount('/content/gdrive')

######
#torch.save(model,PATH) <-- use this to save model weights

#use below function (forward funtion) when you turn it in



#forward function uses "hooks" which are in the nn library.
# we should be using logistic regression for this project. Since it is a classification task
#####
class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(28*28,64) #input layer is 28*28 because it is a 28x28 image
        self.fc2 = nn.Linear(64,64)
        self.fc3 = nn.Linear(64,64)
        # self.dropout1 = nn.Dropout2d(0.50)
        self.conv1 = nn.Conv2d(64, 64, kernel_size=1)
        self.fc4 = nn.Linear(64,10) #2 hidden layers with output layer of 10 (0-9)

    def forward(self, X):
      X = F.relu(self.fc1(X))
      X = F.relu(self.fc2(X))
      # X = self.dropout1(X)
      # X = F.relu(F.max_pool2d(self.conv1(X), 1))
      # X = self.conv1(X)
      # x = F.max_pool2d(x, 1)
      # X = torch.flatten(X, 64)
      X = F.relu(self.fc3(X))
      X = F.log_softmax(self.fc4(X),dim=1)
      return X
    #   #!!can add to this forward function ie sigmoid or reshape
    # src: https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html
    # def __init__(self):
    #   super(NeuralNetwork, self).__init__()
    #   self.conv1 = nn.Conv2d(1, 32, 3, 1)
    #   self.conv2 = nn.Conv2d(32, 64, 3, 1)
    #   self.dropout1 = nn.Dropout2d(0.25)
    #   self.dropout2 = nn.Dropout2d(0.5)
    #   self.fc1 = nn.Linear(64, 64)
    #   self.fc2 = nn.Linear(64, 10)

    # # x represents our data
    # def forward(self, x):
    #   # Pass data through conv1
    #   x = self.conv1(x)
    #   # Use the rectified-linear activation function over x
    #   x = F.relu(x)

    #   x = self.conv2(x)
    #   x = F.relu(x)

    #   # Run max pooling over x
    #   x = F.max_pool2d(x, 13)
    #   # Pass data through dropout1
    #   x = self.dropout1(x)
    #   # Flatten x with start_dim=1
    #   x = torch.flatten(x, 1)
    #   # Pass data through fc1
    #   x = self.fc1(x)
    #   x = F.relu(x)
    #   x = self.dropout2(x)
    #   x = self.fc2(x)

      # Apply softmax to x
      output = F.log_softmax(x, dim=1)
      return output
      
#############
  ## Step 1: Import MNIST Training and Testing Data
mnist_trainset = datasets.MNIST(root='/content/gdrive/MyDrive/Colab Notebooks/ENGR96_ML/MNIST_data', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))
mnist_testset = datasets.MNIST(root='/content/gdrive/MyDrive/Colab Notebooks/ENGR96_ML/MNIST_data', train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))
# print(len(mnist_trainset), len(mnist_testset))
#############
# Viewing info about training and Testing Data
# print(mnist_trainset[1])
# print(mnist_trainset[2])
# print(mnist_trainset[200])
# print(mnist_trainset[5000]) [(img, 9), (img, 8)]
#############
## Step 2 from Bruce's email, load data into tensors
trainset = torch.utils.data.DataLoader(mnist_trainset,batch_size=10,shuffle=True) 
# batch groups testing data, shuffle randomizes data
testset = torch.utils.data.DataLoader(mnist_testset,batch_size=10,shuffle=True) 
############
# Definig model instance, computation on CPU
model = NeuralNetwork().to('cpu')
print(model)

# reshaping 28*28 image to work with nerual network
X = torch.rand([28, 28])
# print(X.shape)
X = X.reshape((1, 28*28))
# X = X.reshape((1, 1, 28, 28))
print("x shape: ", X.shape)
# X = X.reshape((1, 1, 28, 784))
output = model(X)
print(output)
############
NeuralNetwork(
  (fc1): Linear(in_features=784, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
  (fc4): Linear(in_features=64, out_features=10, bias=True)
)
x shape:  torch.Size([1, 784])
tensor([[-2.2444, -2.1968, -2.3757, -2.2591, -2.3629, -2.3351, -2.2998, -2.3653,
         -2.1940, -2.4211]], grad_fn=<LogSoftmaxBackward0>)
#####################
running_loss = []
#use below later for CNN possibly
# number_labels_numpy = np.arange(0,9,1)
# number_labels = torch.tensor(number_labels_numpy)
# one_hot_number_labels = torch.nn.functional.one_hot(number_labels, num_classes = 10)
num_epochs = 10
# batch_size_train = 64
# batch_size_test = 1000
learning_rate = 0.001
momentum = 1
num_epochs_to_print = 1
# log_interval = 10
criterion = nn.CrossEntropyLoss()
#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum) #USING SGD
optimizer = optim.Adam(model.parameters(),lr=learning_rate)
for i in range(num_epochs):
  for data in trainset:
    imag_batch, lbl_batch = data
    optimizer.zero_grad()
    output = model(imag_batch.reshape((-1,784)))
    # output = model(imag_batch)
    # print("output: ", output.shape, output)
    # print("images: ", imag_batch.shape, imag_batch)
    # print("labels: ", lbl_batch.shape, lbl_batch)
    # loss=F.nll_loss(output,lbl_batch)
    loss=criterion(output,lbl_batch)
    running_loss=np.append(running_loss,loss.detach().numpy())
    loss.backward()
    optimizer.step()
  print(loss)
  

  # loss = criterion(outputs, one_hot_number_labels)

  # if i % num_epochs_to_print == 0:
  #   print(loss.item())
t=np.arange(0,6000*num_epochs,1)
plt.title("loss as a function of time")
plt.ylabel("loss")
plt.xlabel("time")
plt.plot(t,running_loss)
####################
t=np.arange(0,60000,1)
plt.title("Loss as a Function of Iterations")
plt.ylabel("Loss")
plt.xlabel("Iterations")
plt.rcParams["figure.figsize"] = (25,15)
# plt.rcParams["figure.figsize"] = (40,10)

plt.plot(t,running_loss)
#####################
# Testing on testing data
correct = 0
total = 0

with torch.no_grad():
  for data in testset:
    img,lbl = data
    output = model(img.reshape((-1,28*28)))
    # output = model(img)
    for idx, i in enumerate(output):
      # print()
      if torch.argmax(i) == lbl[idx]:
        correct += 1
      total += 1

print(correct, total, correct/total)
###################
# Testing on training data
correct = 0
total = 0

with torch.no_grad():
  for data in trainset:
    img,lbl = data
    output = model(img.reshape((-1,28*28)))
    # output = model(img)
    for idx, i in enumerate(output):
      # print()
      if torch.argmax(i) == lbl[idx]:
        correct += 1
      total += 1

print(correct, total, correct/total)
#################
#Save your model weights (this will generate a file that you will have to submit to us)
torch.save(model.state_dict(), '/content/gdrive/MyDrive/Colab Notebooks/ENGR96_ML/Digit_Classifier/weights-8118.pth')
################
testloader = mnist_testset
################
model.eval()
correct = 0
total = 0

#ensure gradients won't get changed
with torch.no_grad():
  for images, labels in testloader:
    # print(images, labels)
    for i in range(labels):
      #do any preprocessing

      #calculate the output
      output = model(images[i])

      predicted_label = output.index(max(output))

      if predicted_label == labels[i]:
        correct += 1
      total += 1

print("Test accuracy: ", correct/total)
###################
